{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b0a1dc-f4fe-4972-9d1e-17e59b46cb1a",
   "metadata": {},
   "source": [
    "<font size=\"6\">Organized Subtask Prompting\n",
    "</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e10673f3-d2e0-4993-a965-9fa199162088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import openai\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f17bed5-124e-4450-b006-430c3c5a9b8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from openai import OpenAI \n",
    "\n",
    "client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key= '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',\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16fa08ca-6a54-40e4-ae2f-4997bc3fa4d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>local_datehour</th>\n",
       "      <th>pid</th>\n",
       "      <th>placename</th>\n",
       "      <th>tag</th>\n",
       "      <th>cat</th>\n",
       "      <th>lat_lng</th>\n",
       "      <th>addr</th>\n",
       "      <th>hash_uid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017-12-09 00</td>\n",
       "      <td>23467</td>\n",
       "      <td>CU</td>\n",
       "      <td>등촌원룸점</td>\n",
       "      <td>Convenience Store</td>\n",
       "      <td>37.5565109631,126.860730127</td>\n",
       "      <td>서울 강서구 등촌동 636</td>\n",
       "      <td>AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2017-12-09 12</td>\n",
       "      <td>86057</td>\n",
       "      <td>서울9호선 증미역</td>\n",
       "      <td>종합운동장 방면 2-4</td>\n",
       "      <td>Subway Station</td>\n",
       "      <td>37.5580690136,126.860647984</td>\n",
       "      <td>서울 강서구 등촌동 666-40</td>\n",
       "      <td>AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2017-12-09 12</td>\n",
       "      <td>86066</td>\n",
       "      <td>서울9호선 신목동역</td>\n",
       "      <td>종합운동장 방면 2-4</td>\n",
       "      <td>Subway Station</td>\n",
       "      <td>37.5441703286,126.88310042</td>\n",
       "      <td>서울 양천구 목동 138-19</td>\n",
       "      <td>AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2017-12-09 13</td>\n",
       "      <td>208705</td>\n",
       "      <td>이맛콩나물국밥</td>\n",
       "      <td>삼성1호점</td>\n",
       "      <td>Korean Food Restaurants</td>\n",
       "      <td>37.5102950657,127.057353344</td>\n",
       "      <td>서울 강남구 삼성동 153-57</td>\n",
       "      <td>AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2017-12-09 17</td>\n",
       "      <td>84592</td>\n",
       "      <td>서울9호선 봉은사역</td>\n",
       "      <td>개화 방면 2-4</td>\n",
       "      <td>Subway Station</td>\n",
       "      <td>37.5142419355,127.06028413</td>\n",
       "      <td>서울 강남구 삼성동 111-8</td>\n",
       "      <td>AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773509</th>\n",
       "      <td>2018-01-02 08</td>\n",
       "      <td>218736</td>\n",
       "      <td>스타필드 코엑스몰점</td>\n",
       "      <td>/코즈니/준오헤어</td>\n",
       "      <td>Outlet/ Shopping Mall</td>\n",
       "      <td>37.5093729161,127.059693374</td>\n",
       "      <td>서울 강남구 삼성동 159-9</td>\n",
       "      <td>//kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773510</th>\n",
       "      <td>2018-01-02 11</td>\n",
       "      <td>255314</td>\n",
       "      <td>유명국양평해장국</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Korean Food Restaurants</td>\n",
       "      <td>37.5164864303,127.018625519</td>\n",
       "      <td>서울 서초구 잠원동 13-12</td>\n",
       "      <td>//kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773511</th>\n",
       "      <td>2018-01-03 15</td>\n",
       "      <td>218727</td>\n",
       "      <td>스타필드 코엑스몰점</td>\n",
       "      <td>/케리마켓/ANLE COFFEE</td>\n",
       "      <td>Outlet/ Shopping Mall</td>\n",
       "      <td>37.509522385,127.05979798</td>\n",
       "      <td>서울 강남구 삼성동 159-9</td>\n",
       "      <td>//kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773512</th>\n",
       "      <td>2018-01-04 15</td>\n",
       "      <td>88341</td>\n",
       "      <td>서울6호선 합정역</td>\n",
       "      <td>응암순환행 6-4</td>\n",
       "      <td>Subway Station</td>\n",
       "      <td>37.5489424459,126.913731247</td>\n",
       "      <td>서울 마포구 합정동 420</td>\n",
       "      <td>//kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>773514</th>\n",
       "      <td>2018-01-04 16</td>\n",
       "      <td>9814</td>\n",
       "      <td>이마트 은평점</td>\n",
       "      <td>/비식품</td>\n",
       "      <td>Discount Department Store</td>\n",
       "      <td>37.6005156078,126.920202412</td>\n",
       "      <td>서울 은평구 응암동 90-1</td>\n",
       "      <td>//kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614401 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       local_datehour     pid   placename                tag  \\\n",
       "2       2017-12-09 00   23467          CU              등촌원룸점   \n",
       "3       2017-12-09 12   86057   서울9호선 증미역       종합운동장 방면 2-4   \n",
       "4       2017-12-09 12   86066  서울9호선 신목동역       종합운동장 방면 2-4   \n",
       "5       2017-12-09 13  208705     이맛콩나물국밥              삼성1호점   \n",
       "6       2017-12-09 17   84592  서울9호선 봉은사역          개화 방면 2-4   \n",
       "...               ...     ...         ...                ...   \n",
       "773509  2018-01-02 08  218736  스타필드 코엑스몰점          /코즈니/준오헤어   \n",
       "773510  2018-01-02 11  255314    유명국양평해장국                NaN   \n",
       "773511  2018-01-03 15  218727  스타필드 코엑스몰점  /케리마켓/ANLE COFFEE   \n",
       "773512  2018-01-04 15   88341   서울6호선 합정역          응암순환행 6-4   \n",
       "773514  2018-01-04 16    9814     이마트 은평점               /비식품   \n",
       "\n",
       "                              cat                      lat_lng  \\\n",
       "2               Convenience Store  37.5565109631,126.860730127   \n",
       "3                  Subway Station  37.5580690136,126.860647984   \n",
       "4                  Subway Station   37.5441703286,126.88310042   \n",
       "5         Korean Food Restaurants  37.5102950657,127.057353344   \n",
       "6                  Subway Station   37.5142419355,127.06028413   \n",
       "...                           ...                          ...   \n",
       "773509      Outlet/ Shopping Mall  37.5093729161,127.059693374   \n",
       "773510    Korean Food Restaurants  37.5164864303,127.018625519   \n",
       "773511      Outlet/ Shopping Mall    37.509522385,127.05979798   \n",
       "773512             Subway Station  37.5489424459,126.913731247   \n",
       "773514  Discount Department Store  37.6005156078,126.920202412   \n",
       "\n",
       "                     addr                                      hash_uid  \n",
       "2          서울 강서구 등촌동 636  AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=  \n",
       "3       서울 강서구 등촌동 666-40  AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=  \n",
       "4        서울 양천구 목동 138-19  AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=  \n",
       "5       서울 강남구 삼성동 153-57  AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=  \n",
       "6        서울 강남구 삼성동 111-8  AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=  \n",
       "...                   ...                                           ...  \n",
       "773509   서울 강남구 삼성동 159-9  //kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=  \n",
       "773510   서울 서초구 잠원동 13-12  //kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=  \n",
       "773511   서울 강남구 삼성동 159-9  //kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=  \n",
       "773512     서울 마포구 합정동 420  //kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=  \n",
       "773514    서울 은평구 응암동 90-1  //kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=  \n",
       "\n",
       "[614401 rows x 8 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('./sample_data_coex_visitors_visits_20171201_20180105.csv', sep=',', skiprows = 3)\n",
    "data = data[~data['addr'].isnull()]\n",
    "data = data[data['addr'].str.contains('서울')]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba75a6c0-8f64-4082-a155-f5970cbc0bf7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hash_uid</th>\n",
       "      <th>date</th>\n",
       "      <th>hour</th>\n",
       "      <th>place</th>\n",
       "      <th>category</th>\n",
       "      <th>address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=</td>\n",
       "      <td>2017-12-09</td>\n",
       "      <td>00</td>\n",
       "      <td>CU 등촌원룸점</td>\n",
       "      <td>Convenience Store</td>\n",
       "      <td>서울 강서구 등촌동 636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=</td>\n",
       "      <td>2017-12-09</td>\n",
       "      <td>12</td>\n",
       "      <td>서울9호선 증미역 종합운동장 방면 2-4</td>\n",
       "      <td>Subway Station</td>\n",
       "      <td>서울 강서구 등촌동 666-40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=</td>\n",
       "      <td>2017-12-09</td>\n",
       "      <td>12</td>\n",
       "      <td>서울9호선 신목동역 종합운동장 방면 2-4</td>\n",
       "      <td>Subway Station</td>\n",
       "      <td>서울 양천구 목동 138-19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=</td>\n",
       "      <td>2017-12-09</td>\n",
       "      <td>13</td>\n",
       "      <td>이맛콩나물국밥 삼성1호점</td>\n",
       "      <td>Korean Food Restaurants</td>\n",
       "      <td>서울 강남구 삼성동 153-57</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=</td>\n",
       "      <td>2017-12-09</td>\n",
       "      <td>17</td>\n",
       "      <td>서울9호선 봉은사역 개화 방면 2-4</td>\n",
       "      <td>Subway Station</td>\n",
       "      <td>서울 강남구 삼성동 111-8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614396</th>\n",
       "      <td>//kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>08</td>\n",
       "      <td>스타필드 코엑스몰점 /코즈니/준오헤어</td>\n",
       "      <td>Outlet/ Shopping Mall</td>\n",
       "      <td>서울 강남구 삼성동 159-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614397</th>\n",
       "      <td>//kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=</td>\n",
       "      <td>2018-01-02</td>\n",
       "      <td>11</td>\n",
       "      <td>유명국양평해장국</td>\n",
       "      <td>Korean Food Restaurants</td>\n",
       "      <td>서울 서초구 잠원동 13-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614398</th>\n",
       "      <td>//kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=</td>\n",
       "      <td>2018-01-03</td>\n",
       "      <td>15</td>\n",
       "      <td>스타필드 코엑스몰점 /케리마켓/ANLE COFFEE</td>\n",
       "      <td>Outlet/ Shopping Mall</td>\n",
       "      <td>서울 강남구 삼성동 159-9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614399</th>\n",
       "      <td>//kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>15</td>\n",
       "      <td>서울6호선 합정역 응암순환행 6-4</td>\n",
       "      <td>Subway Station</td>\n",
       "      <td>서울 마포구 합정동 420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>614400</th>\n",
       "      <td>//kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=</td>\n",
       "      <td>2018-01-04</td>\n",
       "      <td>16</td>\n",
       "      <td>이마트 은평점 /비식품</td>\n",
       "      <td>Discount Department Store</td>\n",
       "      <td>서울 은평구 응암동 90-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>614401 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            hash_uid        date hour  \\\n",
       "0       AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=  2017-12-09   00   \n",
       "1       AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=  2017-12-09   12   \n",
       "2       AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=  2017-12-09   12   \n",
       "3       AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=  2017-12-09   13   \n",
       "4       AACepmlRCBduWs47hBiedMwYEXaoAdQfQ+gZuNjcJnE=  2017-12-09   17   \n",
       "...                                              ...         ...  ...   \n",
       "614396  //kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=  2018-01-02   08   \n",
       "614397  //kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=  2018-01-02   11   \n",
       "614398  //kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=  2018-01-03   15   \n",
       "614399  //kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=  2018-01-04   15   \n",
       "614400  //kIAIl1UY8yg6bTw21m0ubi1fOG1jVal7Z8FDQv19o=  2018-01-04   16   \n",
       "\n",
       "                               place                   category  \\\n",
       "0                           CU 등촌원룸점          Convenience Store   \n",
       "1             서울9호선 증미역 종합운동장 방면 2-4             Subway Station   \n",
       "2            서울9호선 신목동역 종합운동장 방면 2-4             Subway Station   \n",
       "3                      이맛콩나물국밥 삼성1호점    Korean Food Restaurants   \n",
       "4               서울9호선 봉은사역 개화 방면 2-4             Subway Station   \n",
       "...                              ...                        ...   \n",
       "614396          스타필드 코엑스몰점 /코즈니/준오헤어      Outlet/ Shopping Mall   \n",
       "614397                     유명국양평해장국     Korean Food Restaurants   \n",
       "614398  스타필드 코엑스몰점 /케리마켓/ANLE COFFEE      Outlet/ Shopping Mall   \n",
       "614399           서울6호선 합정역 응암순환행 6-4             Subway Station   \n",
       "614400                  이마트 은평점 /비식품  Discount Department Store   \n",
       "\n",
       "                  address  \n",
       "0          서울 강서구 등촌동 636  \n",
       "1       서울 강서구 등촌동 666-40  \n",
       "2        서울 양천구 목동 138-19  \n",
       "3       서울 강남구 삼성동 153-57  \n",
       "4        서울 강남구 삼성동 111-8  \n",
       "...                   ...  \n",
       "614396   서울 강남구 삼성동 159-9  \n",
       "614397   서울 서초구 잠원동 13-12  \n",
       "614398   서울 강남구 삼성동 159-9  \n",
       "614399     서울 마포구 합정동 420  \n",
       "614400    서울 은평구 응암동 90-1  \n",
       "\n",
       "[614401 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_data = pd.DataFrame()\n",
    "new_data['hash_uid'] = data['hash_uid']\n",
    "# new_data['date'] = pd.to_datetime(data['local_datehour'], format='%Y-%m-%d %H').dt.date\n",
    "# new_data['hour'] = pd.to_datetime(data['local_datehour'], format='%Y-%m-%d %H').dt.hour\n",
    "new_data['date'] = data['local_datehour'].str.split(\" \", expand = True)[0]\n",
    "new_data['hour'] = data['local_datehour'].str.split(\" \", expand = True)[1]\n",
    "new_data['place']= data['placename'] + ' ' + data['tag'].fillna('')\n",
    "new_data['category'] = data['cat']\n",
    "new_data['address'] = data['addr']\n",
    "\n",
    "new_data.reset_index(inplace=True, drop=True)\n",
    "new_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12719ff6-e5b0-4ade-ae48-cb6f0162bac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def totext(dataframe):\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "    df = df.sort_values(by=['date', 'hour'])\n",
    "\n",
    "    result = \"A person's visit history from \" + df.loc[0, 'date'][5:10] + df.loc[0, 'date'][4] + df.loc[0, 'date'][0:4] \\\n",
    "       + \" to \" + df.loc[len(df) - 1, 'date'][5:10] + df.loc[len(df) - 1, 'date'][4] + df.loc[len(df) - 1, 'date'][0:4] + \" is as follows:\\n\"\n",
    "    \n",
    "    # result = \"A person's visit history from \" + str(df.loc[0, 'date'].month) + \"-\" + str(df.loc[0, 'date'].day) + \"-\" + str(df.loc[0, 'date'].year) \\\n",
    "    #    + \" to \" + str(df.loc[len(df) - 1, 'date'].month) + \"-\" + str(df.loc[len(df) - 1, 'date'].day) + \"-\" + str(df.loc[len(df) - 1, 'date'].year) + \" is as follows:\\n\"\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        irow = df.iloc[i, :]\n",
    "\n",
    "        if irow['place'][-1] == \" \":\n",
    "            pn = str(irow['place'][:-1])\n",
    "        else:\n",
    "            pn = str(irow['place'])\n",
    "\n",
    "        if i == 0:\n",
    "            result += \"The person visited a \" + irow['category'].lower() + \" named \" + pn + \" located at \" + irow[\n",
    "                'address'] + \" at \" + str(irow['hour']) + \" on \" + irow['date'][5:10] + irow['date'][4] + irow['date'][0:4]\n",
    "        elif i == len(df) - 1:\n",
    "            result += \", and a \" + irow['category'].lower() + \" named \" + pn + \" located at \" + irow['address'] + \" at \" + \\\n",
    "                      str(irow['hour']) + \" on \" + irow['date'][5:10] + irow['date'][4] + irow['date'][0:4] + \".\"\n",
    "        else:\n",
    "            result += \", a \" + irow['category'].lower() + \" named \" + pn + \" located at \" + irow['address'] + \" at \" + \\\n",
    "                      str(irow['hour']) + \" on \" + irow['date'][5:10] + irow['date'][4] + irow['date'][0:4]\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c891bf5-dc74-4205-87c2-5b2241ef7df8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def totextvisited(dataframe):\n",
    "    \n",
    "    df = dataframe.copy()\n",
    "    df = df.sort_values(by = ['date', 'hour'])\n",
    "\n",
    "    result = \"A person's visit history from \" + df.loc[0, 'date'][5:10] + df.loc[0, 'date'][4] + df.loc[0, 'date'][0:4] \\\n",
    "          + \" to \" + df.loc[len(df) - 1, 'date'][5:10] + df.loc[len(df) - 1, 'date'][4] + df.loc[len(df) - 1, 'date'][0:4] + \" is as follows:\\n\"\n",
    "\n",
    "    \n",
    "    visited = {}\n",
    "    visitedcataddr = {}\n",
    "    \n",
    "    for i in range(len(df)):\n",
    "        irow = df.iloc[i, :]\n",
    "        \n",
    "        \n",
    "        if irow['place'][-1] == \" \":\n",
    "        \n",
    "            pn = str(irow['place'][:-1])\n",
    "\n",
    "        else:\n",
    "            pn = str(irow['place'])\n",
    "\n",
    "\n",
    "        \n",
    "        if pn in visited.keys():      \n",
    "            visited[pn] += \", at \" + str(irow['hour']) + \" on \" + irow['date'][5:10] + irow['date'][4] + irow['date'][0:4]\n",
    "            \n",
    "        elif pn not in visited.keys():\n",
    "            visited.update({pn : \" at \" + str(irow['hour']) + \" on \" + irow['date'][5:10] + irow['date'][4] + irow['date'][0:4]})\n",
    "            visitedcataddr.update({pn : [irow['category'].lower(), irow['address']]})\n",
    "            \n",
    "\n",
    "    result += \"The person visited\"\n",
    "    for key, value in visited.items():\n",
    "        if key == list(visited.keys())[-1]:\n",
    "            result += \" a \" + visitedcataddr[key][0] + \" named \" + key \\\n",
    "                    + \" located at \" + visitedcataddr[key][1] + value + \".\"\n",
    "        else:\n",
    "            result += \" a \" + visitedcataddr[key][0] + \" named \" + key \\\n",
    "                    + \" located at \" + visitedcataddr[key][1] + value + \",\"\n",
    "                \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ff752551-d811-49fd-89e0-87c61c0796e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_50plus = new_data['hash_uid'].value_counts()[new_data['hash_uid'].value_counts().values>=50].index\n",
    "select_user_50plus = [user_50plus[i] for i in random.sample(range(0, len(user_50plus)), 50)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ddae539-fd7d-4766-8125-02aae1916aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "duration_ans = [] \n",
    "most_visited_date_ans = []\n",
    "most_visited_hour_ans = []\n",
    "most_visited_timestamp_ans = []\n",
    "longest_time_diff_ans = []\n",
    "\n",
    "for i in select_user_50plus:\n",
    "    # print('----- ',cnt,' -----')\n",
    "    \n",
    "    user_log = new_data[new_data['hash_uid'] == i]\n",
    "    user_log = user_log.drop('hash_uid', axis=1)\n",
    "    user_log = user_log[-50:]\n",
    "    user_log.reset_index(inplace=True, drop=True)\n",
    "    \n",
    "    user_log_shuffle = user_log.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    # 이후 api call에서 저장된 dataframe을 통해 로그를 보낼 예정이라면 따로 저장 필요.\n",
    "    user_log.to_csv(f'./data/subtasks_temporal/user_log/user_log_{cnt}.csv', index=False)\n",
    "    user_log_shuffle.to_csv(f'./data/subtasks_temporal/user_log_shuffle/user_log_shuffle_{cnt}.csv', index=False)\n",
    "    \n",
    "    duration_ans.append(user_log.iloc[0]['date'] + ', ' + user_log.iloc[-1]['date'])\n",
    "    \n",
    "    visit_count_by_date = user_log['date'].value_counts()\n",
    "    most_visited_date = visit_count_by_date.idxmax()\n",
    "    most_visited_date_ans.append(most_visited_date)\n",
    "    \n",
    "    visit_count_by_hour = user_log['hour'].value_counts()\n",
    "    most_visited_hour = visit_count_by_hour.idxmax()\n",
    "    most_visited_hour_ans.append(most_visited_hour)\n",
    "    \n",
    "    visit_count_by_timestamp = user_log.groupby(['date', 'hour']).size().reset_index(name='count')\n",
    "    most_visited_timestamp = visit_count_by_timestamp['count'].idxmax()\n",
    "    most_visited_timestamp_ans.append(visit_count_by_timestamp.iloc[most_visited_timestamp]['date'] + ' ' + visit_count_by_timestamp.iloc[most_visited_timestamp]['hour'])\n",
    "    \n",
    "    timestamp = pd.to_datetime(user_log['date'] + ' ' + user_log['hour'], format='%Y-%m-%d %H')\n",
    "    time_diff = timestamp.diff()\n",
    "    longest_time_diff_ans.append(str(time_diff.max()).split(\" \")[0] + ', ' + str(time_diff.max()).split(\" \")[2][:2])\n",
    "    \n",
    "    cnt += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29eedafb-0b52-40ce-8f66-b2b33b8eee34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# answer를 저장해서 사용하려면 저장 필요\n",
    "np.save('./data/subtasks_temporal/duration_ans', duration_ans)\n",
    "np.save('./data/subtasks_temporal/most_visited_date_ans', most_visited_date_ans)\n",
    "np.save('./data/subtasks_temporal/most_visited_hour_ans', most_visited_hour_ans)\n",
    "np.save('./data/subtasks_temporal/most_visited_timestamp_ans', most_visited_timestamp_ans)\n",
    "np.save('./data/subtasks_temporal/longest_time_diff_ans', longest_time_diff_ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "d64ad820-a2b0-46bb-915d-f598d797cd18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gpt_api(system_prompt, user_prompt, user_log, model=\"gpt-3.5-turbo\", verbose=False):\n",
    "\n",
    "    user_prompt += '\\n' + user_log\n",
    "    \n",
    "    if verbose:\n",
    "      print(user_prompt)\n",
    "\n",
    "    messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt}, \n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "                ]\n",
    "    \n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     model=model,\n",
    "    #     messages=messages,\n",
    "    #     temperature=0,\n",
    "    # )\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    output = response.choices[0].message.content\n",
    "    token = response.usage.total_tokens\n",
    "    \n",
    "    print('answer : ', output)\n",
    "    # print('token : ', token)\n",
    "\n",
    "    return output, token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3269e85-ab79-47ab-824d-d3a9616aae09",
   "metadata": {},
   "source": [
    "<font size=\"5\">Task 1</font>\n",
    "\n",
    "데이터 인풋 형태는 dataframe, 순서가 뒤섞인 dataframe, text로 변환된 형태 두 개(totext(), totextvisited())를 사용합니다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "97f3ab09-b986-42ab-8417-d4e1ae86b505",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_pred_df = []\n",
    "duration_pred_df_shuf = []\n",
    "duration_pred_text1 = []\n",
    "duration_pred_text2 = []\n",
    "\n",
    "system_prompt = 'You only answer in the following format: year-month-day, year-month-day'\n",
    "user_prompt = 'What is the time range covered by this visit log?'\n",
    "\n",
    "for i in range(50):\n",
    "    # print('original df')\n",
    "    user_log = pd.read_csv(f'./data/subtasks_temporal/user_log/user_log_{i}.csv')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, user_log.to_string())\n",
    "    duration_pred_df.append(output)\n",
    "    \n",
    "    # print('shuffle df')\n",
    "    user_log_shuffle = pd.read_csv(f'./data/subtasks_temporal/user_log_shuffle/user_log_shuffle_{i}.csv')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, user_log_shuffle.to_string())\n",
    "    duration_pred_df_shuf.append(output)\n",
    "    \n",
    "    # print('text ver1')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, totext(user_log))\n",
    "    duration_pred_text1.append(output)\n",
    "    \n",
    "    # print('text ver2')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, totextvisited(user_log))\n",
    "    duration_pred_text2.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b8b87cc-d270-4602-9965-4c9c0ef52dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "df shuffle\n",
      "text1\n",
      "text2\n"
     ]
    }
   ],
   "source": [
    "duration_pred_df_score = []\n",
    "\n",
    "print('df')\n",
    "for ai, bi in zip(duration_ans, duration_pred_df):\n",
    "    duration_pred_df_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "duration_pred_df_shuf_score = []\n",
    "\n",
    "print('df shuffle')\n",
    "for ai, bi in zip(duration_ans, duration_pred_df_shuf):\n",
    "    duration_pred_df_shuf_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "duration_pred_df_text1_score = []\n",
    "\n",
    "print('text1')\n",
    "for ai, bi in zip(duration_ans, duration_pred_text1):\n",
    "    duration_pred_df_text1_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "duration_pred_df_text2_score = []\n",
    "\n",
    "print('text2')\n",
    "for ai, bi in zip(duration_ans, duration_pred_text2):\n",
    "    duration_pred_df_text2_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e09e6c6-25e4-4abf-afdd-be3eb3afde71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(sum(duration_pred_df_score)/50)\n",
    "print(sum(duration_pred_df_shuf_score)/50)\n",
    "print(sum(duration_pred_df_text1_score)/50)\n",
    "print(sum(duration_pred_df_text2_score)/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b518683-ac43-42cc-b375-23503004163f",
   "metadata": {},
   "source": [
    "<font size=\"5\">Task 2</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57cbf6d3-1b0e-47db-a766-13c1d62f5ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_visited_date_pred_df = []\n",
    "most_visited_date_pred_df_shuf = []\n",
    "most_visited_date_pred_text1 = []\n",
    "most_visited_date_pred_text2 = []\n",
    "\n",
    "system_prompt = 'You only answer in the following format: year-month-day'\n",
    "user_prompt = 'When is the date with the most visits? However, if you have several days with the most visits, answer the earliest date.'\n",
    "\n",
    "for i in range(50):\n",
    "    # print('------', i, '-------')\n",
    "    # print('original df')\n",
    "    user_log = pd.read_csv(f'./data/subtasks_temporal/user_log/user_log_{i}.csv')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, user_log.to_string())\n",
    "    most_visited_date_pred_df.append(output)\n",
    "    \n",
    "    # print('shuffle df')\n",
    "    user_log_shuffle = pd.read_csv(f'./data/subtasks_temporal/user_log_shuffle/user_log_shuffle_{i}.csv')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, user_log_shuffle.to_string())\n",
    "    most_visited_date_pred_df_shuf.append(output)\n",
    "    \n",
    "    # print('text ver1')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, totext(user_log))\n",
    "    most_visited_date_pred_text1.append(output)\n",
    "    \n",
    "    # print('text ver2')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, totextvisited(user_log))\n",
    "    most_visited_date_pred_text2.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd9f40b4-2c4d-4760-8d5d-2886419a171d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "2017-12-09 2017-12-08 False\n",
      "2017-12-20 2017-12-09 False\n",
      "2017-12-11 2017-12-10 False\n",
      "2017-12-23 2017-12-16 False\n",
      "2017-12-07 2017-12-05 False\n",
      "2017-12-22 2017-12-13 False\n",
      "2017-12-16 2017-12-05 False\n",
      "2017-12-29 2017-12-17 False\n",
      "2017-12-18 2017-12-07 False\n",
      "2018-01-04 2018-01-03 False\n",
      "2017-12-23 2017-12-12 False\n",
      "2017-12-18 2017-12-16 False\n",
      "2018-01-03 2017-12-29 False\n",
      "2017-12-26 2017-12-04 False\n",
      "2017-12-23 2017-12-16 False\n",
      "2017-12-27 2017-12-26 False\n",
      "2017-12-20 2017-12-08 False\n",
      "2017-12-28 2017-12-22 False\n",
      "2017-12-31 2017-12-16 False\n",
      "2017-12-14 2017-12-05 False\n",
      "2017-12-09 2017-12-03 False\n",
      "2017-12-25 2017-12-16 False\n",
      "2017-12-19 2017-12-06 False\n",
      "2017-12-30 2017-12-20 False\n",
      "2017-12-28 2017-12-26 False\n",
      "2018-01-02 2017-12-18 False\n",
      "2018-01-05 2017-12-08 False\n",
      "2017-12-30 2017-12-22 False\n",
      "2018-01-01 2017-12-13 False\n",
      "2018-01-01 2017-12-17 False\n",
      "2017-12-23 2017-12-05 False\n",
      "2017-12-15 2017-12-04 False\n",
      "2017-12-29 2017-12-25 False\n",
      "2018-01-05 2017-12-26 False\n",
      "2017-12-21 2017-12-15 False\n",
      "2017-12-25 2017-12-21 False\n",
      "2017-12-27 2017-12-22 False\n",
      "2018-01-04 2017-12-21 False\n",
      "2017-12-28 2017-12-25 False\n",
      "2017-12-27 2017-12-19 False\n",
      "df_shuffle\n",
      "2017-12-09 2017-12-08 False\n",
      "2017-12-20 2017-12-29 False\n",
      "2017-12-11 2017-12-10 False\n",
      "2017-12-22 2017-12-15 False\n",
      "2018-01-04 2018-01-03 False\n",
      "2018-01-03 2017-12-29 False\n",
      "2017-12-26 2017-12-06 False\n",
      "2017-12-23 2017-12-16 False\n",
      "2017-12-27 2017-12-26 False\n",
      "2017-12-28 2017-12-27 False\n",
      "2017-12-09 2017-12-03 False\n",
      "2017-12-25 2017-12-18 False\n",
      "2017-12-19 2017-12-08 False\n",
      "2017-12-28 2017-12-26 False\n",
      "2018-01-02 2017-12-31 False\n",
      "2018-01-05 2017-12-11 False\n",
      "2018-01-01 2017-12-13 False\n",
      "2018-01-01 2017-12-17 False\n",
      "2018-01-05 2017-12-27 False\n",
      "2017-12-21 2017-12-17 False\n",
      "2017-12-27 2017-12-26 False\n",
      "2018-01-04 2017-12-21 False\n",
      "2017-12-28 2017-12-25 False\n",
      "2017-12-27 2017-12-20 False\n",
      "df_text1\n",
      "2017-12-09 2017-12-08 False\n",
      "2017-12-20 2017-12-29 False\n",
      "2017-12-22 2017-12-15 False\n",
      "2017-12-26 2017-12-06 False\n",
      "2017-12-27 2017-12-26 False\n",
      "2017-12-20 2017-12-26 False\n",
      "2017-12-28 2017-12-23 False\n",
      "2017-12-31 2017-12-21 False\n",
      "2017-12-30 2017-12-20 False\n",
      "2017-12-28 2017-12-26 False\n",
      "2018-01-02 2017-12-18 False\n",
      "2018-01-05 2017-12-11 False\n",
      "2018-01-01 2017-12-13 False\n",
      "2018-01-01 2017-12-31 False\n",
      "2017-12-29 2017-12-28 False\n",
      "2018-01-05 2017-12-26 False\n",
      "2017-12-21 2017-12-19 False\n",
      "2017-12-27 2017-12-24 False\n",
      "2018-01-04 2017-12-27 False\n",
      "df_text2\n",
      "2017-12-09 2017-12-08 False\n",
      "2017-12-20 2017-12-29 False\n",
      "2017-12-29 The date with the most visits is 12-29-2017. False\n",
      "2018-01-03 2017-12-29 False\n",
      "2017-12-26 2017-12-06 False\n",
      "2017-12-27 2017-12-26 False\n",
      "2017-12-20 2017-12-26 False\n",
      "2017-12-28 2017-12-23 False\n",
      "2017-12-31 2017-12-21 False\n",
      "2017-12-09 2017-12-03 False\n",
      "2017-12-28 2017-12-26 False\n",
      "2018-01-02 2017-12-18 False\n",
      "2018-01-05 2017-12-11 False\n",
      "2018-01-01 2017-12-13 False\n",
      "2018-01-01 2017-12-17 False\n",
      "2017-12-15 The date with the most visits is 2017-12-14. False\n",
      "2018-01-05 2017-12-26 False\n",
      "2017-12-21 2017-12-19 False\n",
      "2017-12-27 2017-12-24 False\n",
      "2018-01-04 2017-12-24 False\n",
      "2017-12-28 2017-12-25 False\n"
     ]
    }
   ],
   "source": [
    "most_visited_date_pred_df_score = []\n",
    "\n",
    "print('df')\n",
    "for ai, bi in zip(most_visited_date_ans, most_visited_date_pred_df):\n",
    "    most_visited_date_pred_df_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_date_pred_df_shuf_score = []\n",
    "\n",
    "print('df_shuffle')\n",
    "for ai, bi in zip(most_visited_date_ans, most_visited_date_pred_df_shuf):\n",
    "    most_visited_date_pred_df_shuf_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_date_pred_text1_score = []\n",
    "\n",
    "print('df_text1')\n",
    "for ai, bi in zip(most_visited_date_ans, most_visited_date_pred_text1):\n",
    "    most_visited_date_pred_text1_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_date_pred_text2_score = []\n",
    "\n",
    "print('df_text2')\n",
    "for ai, bi in zip(most_visited_date_ans, most_visited_date_pred_text2):\n",
    "    most_visited_date_pred_text2_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bdb814d8-393a-49f1-8bd0-dcd150c94fff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "0.52\n",
      "0.62\n",
      "0.58\n"
     ]
    }
   ],
   "source": [
    "print(sum(most_visited_date_pred_df_score)/50)\n",
    "print(sum(most_visited_date_pred_df_shuf_score)/50)\n",
    "print(sum(most_visited_date_pred_text1_score)/50)\n",
    "print(sum(most_visited_date_pred_text2_score)/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d509345-66e5-4b49-9697-94f80cd07dbd",
   "metadata": {},
   "source": [
    "<font size=\"5\">Task 5</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d753ff42-6680-474f-bc57-0921d6eb073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_visited_hour_pred_df = []\n",
    "most_visited_hour_pred_df_shuf = []\n",
    "most_visited_hour_pred_text1 = []\n",
    "most_visited_hour_pred_text2 = []\n",
    "\n",
    "system_prompt = 'You only answer in the one number: hour. Dont answer with a sentence.'\n",
    "user_prompt = 'When is the hour with the most visits? However, if you have several hour with the most visits, answer the earliest hour.'\n",
    "\n",
    "for i in range(50):\n",
    "    # print('------', i, '-------')\n",
    "    # print('original df')\n",
    "    user_log = pd.read_csv(f'./data/subtasks_temporal/user_log/user_log_{i}.csv')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, user_log.to_string())\n",
    "    most_visited_hour_pred_df.append(output)\n",
    "    \n",
    "    # print('shuffle df')\n",
    "    user_log_shuffle = pd.read_csv(f'./data/subtasks_temporal/user_log_shuffle/user_log_shuffle_{i}.csv')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, user_log_shuffle.to_string())\n",
    "    most_visited_hour_pred_df_shuf.append(output)\n",
    "    \n",
    "    # print('text ver1')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, totext(user_log))\n",
    "    most_visited_hour_pred_text1.append(output)\n",
    "    \n",
    "    # print('text ver2')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, totextvisited(user_log))\n",
    "    most_visited_hour_pred_text2.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "62768501-9eb1-4fce-a2cd-32c3b9e5bf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "14 18 False\n",
      "18 21 False\n",
      "16 18 False\n",
      "22 17 False\n",
      "12 14 False\n",
      "12 16 False\n",
      "14 19 False\n",
      "12 21 False\n",
      "7 14 False\n",
      "18 21 False\n",
      "16 19 False\n",
      "6 21 False\n",
      "14 17 False\n",
      "15 17 False\n",
      "7 22 False\n",
      "18 23 False\n",
      "8 17 False\n",
      "19 16 False\n",
      "15 16 False\n",
      "18 19 False\n",
      "19 18 False\n",
      "df shuffle\n",
      "16 21 False\n",
      "17 20 False\n",
      "12 16 False\n",
      "14 22 False\n",
      "16 18 False\n",
      "14 23 False\n",
      "15 17 False\n",
      "17 18 False\n",
      "15 11 False\n",
      "18 21 False\n",
      "7 22 False\n",
      "12 18 False\n",
      "17 21 False\n",
      "19 21 False\n",
      "text1\n",
      "14 20 False\n",
      "18 21 False\n",
      "22 17 False\n",
      "12 14 False\n",
      "17 18 False\n",
      "14 19 False\n",
      "12 21 False\n",
      "16 19 False\n",
      "19 15 False\n",
      "6 20 False\n",
      "14 8 False\n",
      "12 8 False\n",
      "15 17 False\n",
      "17 18 False\n",
      "15 16 False\n",
      "7 20 False\n",
      "8 17 False\n",
      "11 9 False\n",
      "19 16 False\n",
      "19 16 False\n",
      "14 13 False\n",
      "text2\n",
      "14 20 False\n",
      "22 17 False\n",
      "12 9 False\n",
      "17 18 False\n",
      "14 19 False\n",
      "16 17 False\n",
      "6 20 False\n",
      "14 8 False\n",
      "15 17 False\n",
      "17 18 False\n",
      "8 17 False\n",
      "11 8 False\n",
      "19 16 False\n",
      "19 21 False\n"
     ]
    }
   ],
   "source": [
    "most_visited_hour_pred_df_score = []\n",
    "\n",
    "print('df')\n",
    "for ai, bi in zip(list(map(int, most_visited_hour_ans)), list(map(int, most_visited_hour_pred_df))):\n",
    "    most_visited_hour_pred_df_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_hour_pred_df_shuf_score = []\n",
    "\n",
    "print('df shuffle')\n",
    "for ai, bi in zip(list(map(int, most_visited_hour_ans)), list(map(int, most_visited_hour_pred_df_shuf))):\n",
    "    most_visited_hour_pred_df_shuf_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_hour_pred_text1_score = []\n",
    "\n",
    "print('text1')\n",
    "for ai, bi in zip(list(map(int, most_visited_hour_ans)), list(map(int, most_visited_hour_pred_text1))):\n",
    "    most_visited_hour_pred_text1_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_hour_pred_text2_score = []\n",
    "\n",
    "print('text2')\n",
    "for ai, bi in zip(list(map(int, most_visited_hour_ans)), list(map(int, most_visited_hour_pred_text2))):\n",
    "    most_visited_hour_pred_text2_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8a0a375d-37df-453d-ae93-1361d62c2dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.58\n",
      "0.72\n",
      "0.58\n",
      "0.72\n"
     ]
    }
   ],
   "source": [
    "print(sum(most_visited_hour_pred_df_score)/50)\n",
    "print(sum(most_visited_hour_pred_df_shuf_score)/50)\n",
    "print(sum(most_visited_hour_pred_text1_score)/50)\n",
    "print(sum(most_visited_hour_pred_text2_score)/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c54c0bc-07fe-4f65-a436-7979c164766f",
   "metadata": {},
   "source": [
    "<font size=\"5\">Task 4</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c3629b10-ffc4-4ffc-8749-5b4e6a097614",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_visited_timestamp_pred_df = []\n",
    "most_visited_timestamp_pred_df_shuf = []\n",
    "most_visited_timestamp_pred_text1 = []\n",
    "most_visited_timestamp_pred_text2 = []\n",
    "\n",
    "system_prompt = 'You only answer in the following format: year-mont-day hour. Dont answer with a sentence.'\n",
    "user_prompt = 'When is the timestamp with the most visits? However, if you have several timstamp with the most visits, answer the earliest one.'\n",
    "\n",
    "for i in range(50):\n",
    "    # print('------', i, '-------')\n",
    "    # print('original df')\n",
    "    user_log = pd.read_csv(f'./data/subtasks_temporal/user_log/user_log_{i}.csv')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, user_log.to_string())\n",
    "    most_visited_timestamp_pred_df.append(output)\n",
    "    \n",
    "    # print('shuffle df')\n",
    "    user_log_shuffle = pd.read_csv(f'./data/subtasks_temporal/user_log_shuffle/user_log_shuffle_{i}.csv')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, user_log_shuffle.to_string())\n",
    "    most_visited_timestamp_pred_df_shuf.append(output)\n",
    "    \n",
    "    # print('text ver1')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, totext(user_log))\n",
    "    most_visited_timestamp_pred_text1.append(output)\n",
    "    \n",
    "    # print('text ver2')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, totextvisited(user_log))\n",
    "    most_visited_timestamp_pred_text2.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a511220f-ddfd-436f-847a-7bc5ea0dd228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8d9d8f74-b5fc-4b69-bb51-0db1ebc82604",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "2017-12-09 14:00:00 2017-12-08 18:00:00 False\n",
      "2017-12-20 23:00:00 2017-12-11 10:00:00 False\n",
      "2017-12-11 16:00:00 2017-12-10 17:00:00 False\n",
      "2017-12-23 17:00:00 2017-12-16 14:00:00 False\n",
      "2017-12-13 23:00:00 2017-12-05 12:00:00 False\n",
      "2017-12-14 18:00:00 2017-12-13 12:00:00 False\n",
      "2017-12-16 17:00:00 2017-12-05 16:00:00 False\n",
      "2017-12-29 12:00:00 2017-12-17 14:00:00 False\n",
      "2017-12-18 12:00:00 2017-12-07 16:00:00 False\n",
      "2018-01-05 22:00:00 2018-01-03 19:00:00 False\n",
      "2017-12-23 18:00:00 2017-12-23 14:00:00 False\n",
      "2017-12-19 15:00:00 2017-12-16 21:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-29 17:00:00 False\n",
      "2017-12-06 21:00:00 2017-12-04 11:00:00 False\n",
      "2017-12-23 14:00:00 2017-12-16 03:00:00 False\n",
      "2017-12-20 07:00:00 2017-12-08 20:00:00 False\n",
      "2017-12-20 19:00:00 2017-12-20 15:00:00 False\n",
      "2017-12-23 16:00:00 2017-12-22 06:00:00 False\n",
      "2017-12-23 22:00:00 2017-12-21 08:00:00 False\n",
      "2017-12-14 15:00:00 2017-12-05 15:00:00 False\n",
      "2017-12-30 17:00:00 2017-12-30 15:00:00 False\n",
      "2017-12-25 13:00:00 2017-12-16 22:00:00 False\n",
      "2017-12-19 22:00:00 2017-12-06 09:00:00 False\n",
      "2017-12-30 15:00:00 2017-12-20 11:00:00 False\n",
      "2017-12-13 06:00:00 2017-12-09 09:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-08 23:00:00 False\n",
      "2017-12-30 17:00:00 2017-12-22 18:00:00 False\n",
      "2017-12-08 23:00:00 2017-12-08 12:00:00 False\n",
      "2018-01-01 16:00:00 2017-12-13 16:00:00 False\n",
      "2018-01-01 19:00:00 2017-12-17 17:00:00 False\n",
      "2017-12-05 12:00:00 2017-12-04 12:00:00 False\n",
      "2017-12-29 21:00:00 2017-12-25 17:00:00 False\n",
      "2017-12-22 19:00:00 2017-12-22 16:00:00 False\n",
      "2017-12-29 19:00:00 2017-12-09 16:00:00 False\n",
      "2018-01-05 17:00:00 2017-12-26 08:00:00 False\n",
      "2017-12-25 14:00:00 2017-12-21 12:00:00 False\n",
      "2017-12-29 15:00:00 2017-12-22 10:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-21 19:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-25 09:00:00 False\n",
      "2017-12-02 18:00:00 2017-12-02 16:00:00 False\n",
      "2017-12-22 18:00:00 2017-12-19 12:00:00 False\n",
      "df shuffle\n",
      "2017-12-09 14:00:00 2017-12-08 20:00:00 False\n",
      "2017-12-20 23:00:00 2017-12-29 19:00:00 False\n",
      "2017-12-11 16:00:00 2017-12-11 15:00:00 False\n",
      "2017-12-23 17:00:00 2017-12-23 21:00:00 False\n",
      "2017-12-13 23:00:00 2017-12-07 18:00:00 False\n",
      "2017-12-14 18:00:00 2017-12-15 14:00:00 False\n",
      "2017-12-16 17:00:00 2017-12-16 20:00:00 False\n",
      "2017-12-18 12:00:00 2017-12-18 13:00:00 False\n",
      "2018-01-05 22:00:00 2018-01-03 20:00:00 False\n",
      "2017-12-23 18:00:00 2017-12-23 20:00:00 False\n",
      "2017-12-19 15:00:00 2017-12-17 11:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-29 17:00:00 False\n",
      "2017-12-06 21:00:00 2017-12-16 18:00:00 False\n",
      "2017-12-20 07:00:00 2017-12-08 20:00:00 False\n",
      "2017-12-23 16:00:00 2017-12-22 06:00:00 False\n",
      "2017-12-23 22:00:00 2017-12-21 08:00:00 False\n",
      "2017-12-03 18:00:00 2017-12-12 12:00:00 False\n",
      "2017-12-25 13:00:00 2017-12-18 17:00:00 False\n",
      "2017-12-19 22:00:00 2017-12-08 12:00:00 False\n",
      "2017-12-05 18:00:00 2017-12-06 17:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-11 19:00:00 False\n",
      "2017-12-30 17:00:00 2017-12-30 19:00:00 False\n",
      "2018-01-01 16:00:00 2017-12-13 14:00:00 False\n",
      "2018-01-01 19:00:00 2017-12-17 17:00:00 False\n",
      "2017-12-05 09:00:00 2017-12-23 11:00:00 False\n",
      "2017-12-05 12:00:00 2017-12-15 11:00:00 False\n",
      "2017-12-29 19:00:00 2017-12-09 17:00:00 False\n",
      "2018-01-05 17:00:00 2017-12-27 09:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-21 20:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-25 15:00:00 False\n",
      "2017-12-02 18:00:00 2017-12-02 16:00:00 False\n",
      "2017-12-22 18:00:00 2017-12-19 07:00:00 False\n",
      "text1\n",
      "2017-12-09 14:00:00 2017-12-08 20:00:00 False\n",
      "2017-12-20 23:00:00 2017-12-12 21:00:00 False\n",
      "2017-12-11 16:00:00 2017-12-10 17:00:00 False\n",
      "2017-12-13 23:00:00 2017-12-07 17:00:00 False\n",
      "2017-12-14 18:00:00 2017-12-13 14:00:00 False\n",
      "2017-12-16 17:00:00 2017-12-05 16:00:00 False\n",
      "2017-12-29 12:00:00 2017-12-17 14:00:00 False\n",
      "2018-01-05 22:00:00 2018-01-04 14:00:00 False\n",
      "2017-12-19 15:00:00 2017-12-16 21:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-29 17:00:00 False\n",
      "2017-12-23 14:00:00 2017-12-16 17:00:00 False\n",
      "2017-12-20 07:00:00 2017-12-08 21:00:00 False\n",
      "2017-12-20 19:00:00 2017-12-20 15:00:00 False\n",
      "2017-12-23 16:00:00 2017-12-22 20:00:00 False\n",
      "2017-12-23 22:00:00 2017-12-21 08:00:00 False\n",
      "2017-12-14 15:00:00 2017-12-05 17:00:00 False\n",
      "2017-12-25 13:00:00 2017-12-18 17:00:00 False\n",
      "2017-12-19 22:00:00 2017-12-19 12:00:00 False\n",
      "2017-12-30 15:00:00 2017-12-20 18:00:00 False\n",
      "2017-12-13 06:00:00 2017-12-09 09:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-11 19:00:00 False\n",
      "2017-12-30 17:00:00 2017-12-30 22:00:00 False\n",
      "2018-01-01 16:00:00 2017-12-13 16:00:00 False\n",
      "2018-01-01 19:00:00 2017-12-17 17:00:00 False\n",
      "2017-12-05 09:00:00 2017-12-07 09:00:00 False\n",
      "2017-12-05 12:00:00 2017-12-04 12:00:00 False\n",
      "2017-12-29 21:00:00 2017-12-25 17:00:00 False\n",
      "2017-12-22 19:00:00 2017-12-22 16:00:00 False\n",
      "2017-12-29 19:00:00 2017-12-09 16:00:00 False\n",
      "2018-01-05 17:00:00 2017-12-26 18:00:00 False\n",
      "2017-12-25 14:00:00 2017-12-25 13:00:00 False\n",
      "2017-12-29 15:00:00 2017-12-24 15:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-21 20:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-25 15:00:00 False\n",
      "2017-12-22 18:00:00 2017-12-19 19:00:00 False\n",
      "text2\n",
      "2017-12-09 14:00:00 2017-12-08 20:00:00 False\n",
      "2017-12-20 23:00:00 2017-12-12 19:00:00 False\n",
      "2017-12-11 16:00:00 2017-12-10 17:00:00 False\n",
      "2017-12-13 23:00:00 2017-12-07 14:00:00 False\n",
      "2017-12-14 18:00:00 2017-12-13 13:00:00 False\n",
      "2017-12-16 17:00:00 2017-12-05 16:00:00 False\n",
      "2017-12-29 12:00:00 2017-12-18 13:00:00 False\n",
      "2018-01-05 22:00:00 2018-01-03 19:00:00 False\n",
      "2017-12-19 15:00:00 2017-12-16 21:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-29 17:00:00 False\n",
      "2017-12-06 21:00:00 2017-12-04 17:00:00 False\n",
      "2017-12-23 14:00:00 2017-12-16 17:00:00 False\n",
      "2017-12-20 07:00:00 2017-12-08 20:00:00 False\n",
      "2017-12-23 16:00:00 2017-12-22 20:00:00 False\n",
      "2017-12-23 22:00:00 2017-12-21 08:00:00 False\n",
      "2017-12-14 15:00:00 2017-12-14 17:00:00 False\n",
      "2017-12-25 13:00:00 2017-12-16 22:00:00 False\n",
      "2017-12-19 22:00:00 2017-12-06 18:00:00 False\n",
      "2017-12-13 06:00:00 2017-12-09 09:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-11 07:00:00 False\n",
      "2017-12-30 17:00:00 2017-12-30 19:00:00 False\n",
      "2018-01-01 16:00:00 2017-12-13 16:00:00 False\n",
      "2018-01-01 19:00:00 2017-12-17 17:00:00 False\n",
      "2017-12-05 09:00:00 2017-12-07 09:00:00 False\n",
      "2017-12-05 12:00:00 2017-12-27 16:00:00 False\n",
      "2017-12-29 21:00:00 2017-12-25 17:00:00 False\n",
      "2017-12-29 19:00:00 2017-12-09 16:00:00 False\n",
      "2018-01-05 17:00:00 2017-12-26 08:00:00 False\n",
      "2017-12-29 15:00:00 2017-12-24 15:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-21 20:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-25 15:00:00 False\n",
      "2017-12-22 18:00:00 2017-12-19 07:00:00 False\n"
     ]
    }
   ],
   "source": [
    "most_visited_timestamp_pred_df_score = []\n",
    "\n",
    "print('df')\n",
    "for ai, bi in zip([dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_ans], [dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_pred_df]):\n",
    "    most_visited_timestamp_pred_df_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_timestamp_pred_df_shuf_score = []\n",
    "\n",
    "print('df shuffle')\n",
    "for ai, bi in zip([dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_ans], [dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_pred_df_shuf]):\n",
    "    most_visited_timestamp_pred_df_shuf_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_timestamp_pred_text1_score = []\n",
    "\n",
    "print('text1')\n",
    "for ai, bi in zip([dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_ans], [dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_pred_text1]):\n",
    "    most_visited_timestamp_pred_text1_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_timestamp_pred_text2_score = []\n",
    "\n",
    "print('text2')\n",
    "for ai, bi in zip([dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_ans], [dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_pred_text2]):\n",
    "    most_visited_timestamp_pred_text2_score.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5ede2799-a381-4976-b788-a5dc1ccf03d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18\n",
      "0.36\n",
      "0.3\n",
      "0.36\n"
     ]
    }
   ],
   "source": [
    "print(sum(most_visited_timestamp_pred_df_score)/50)\n",
    "print(sum(most_visited_timestamp_pred_df_shuf_score)/50)\n",
    "print(sum(most_visited_timestamp_pred_text1_score)/50)\n",
    "print(sum(most_visited_timestamp_pred_text2_score)/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e64ae9-a83a-44f8-9161-18d0c39a734f",
   "metadata": {},
   "source": [
    "<font size=\"5\">Task 5</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bb2e73f0-6209-4977-aa73-e69a989d4612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ 0 -------\n",
      "answer :  1 day\n",
      "answer :  1 day\n",
      "answer :  1 day\n",
      "answer :  1 day\n",
      "------ 1 -------\n",
      "answer :  1 day\n",
      "answer :  10 days\n",
      "answer :  7 days\n",
      "answer :  17 days\n",
      "------ 2 -------\n",
      "answer :  2 days\n",
      "answer :  25 days\n",
      "answer :  16 days\n",
      "answer :  16 days\n",
      "------ 3 -------\n",
      "answer :  10 days\n",
      "answer :  7 days\n",
      "answer :  11 days\n",
      "answer :  11 days\n",
      "------ 4 -------\n",
      "answer :  2 days\n",
      "answer :  24 days\n",
      "answer :  2 days\n",
      "answer :  9 days\n",
      "------ 5 -------\n",
      "answer :  3 days\n",
      "answer :  20 days\n",
      "answer :  3 days\n",
      "answer :  3 days\n",
      "------ 6 -------\n",
      "answer :  2 days\n",
      "answer :  19 days\n",
      "answer :  19 days\n",
      "answer :  19 days\n",
      "------ 7 -------\n",
      "answer :  4 days\n",
      "answer :  7 days\n",
      "answer :  3 days\n",
      "answer :  10 days\n",
      "------ 8 -------\n",
      "answer :  6 days\n",
      "answer :  1 day\n",
      "answer :  10 days\n",
      "answer :  10 days\n",
      "------ 9 -------\n",
      "answer :  2 days\n",
      "answer :  2 days\n",
      "answer :  2 days\n",
      "answer :  2 days\n",
      "------ 10 -------\n",
      "answer :  3 days\n",
      "answer :  13 days\n",
      "answer :  10 days\n",
      "answer :  13 days\n",
      "------ 11 -------\n",
      "answer :  2 days\n",
      "answer :  11 days\n",
      "answer :  2 days\n",
      "answer :  5 days\n",
      "------ 12 -------\n",
      "answer :  2 days\n",
      "answer :  3 days\n",
      "answer :  2 days\n",
      "answer :  7 days\n",
      "------ 13 -------\n",
      "answer :  10 days\n",
      "answer :  20 days\n",
      "answer :  10 days\n",
      "answer :  9 days\n",
      "------ 14 -------\n",
      "answer :  9 days\n",
      "answer :  7 days\n",
      "answer :  9 days\n",
      "answer :  9 days\n",
      "------ 15 -------\n",
      "answer :  3 days\n",
      "answer :  9 days\n",
      "answer :  9 days\n",
      "answer :  9 days\n",
      "------ 16 -------\n",
      "answer :  3 days\n",
      "answer :  6 days\n",
      "answer :  7 days\n",
      "answer :  17 days\n",
      "------ 17 -------\n",
      "answer :  9 days\n",
      "answer :  12 days\n",
      "answer :  3 days\n",
      "answer :  10 days\n",
      "------ 18 -------\n",
      "answer :  2 days\n",
      "answer :  10 days\n",
      "answer :  5 days\n",
      "answer :  4 days\n",
      "------ 19 -------\n",
      "answer :  6 days\n",
      "answer :  9 days\n",
      "answer :  7 days\n",
      "answer :  7 days\n",
      "------ 20 -------\n",
      "answer :  3 days\n",
      "answer :  4 days\n",
      "answer :  9 days\n",
      "answer :  9 days\n",
      "------ 21 -------\n",
      "answer :  3 days\n",
      "answer :  19 days\n",
      "answer :  9 days\n",
      "answer :  9 days\n",
      "------ 22 -------\n",
      "answer :  2 days\n",
      "answer :  6 days\n",
      "answer :  1 day\n",
      "answer :  2 days\n",
      "------ 23 -------\n",
      "answer :  3 days\n",
      "answer :  12 days\n",
      "answer :  3 days\n",
      "answer :  13 days\n",
      "------ 24 -------\n",
      "answer :  7 days\n",
      "answer :  8 days\n",
      "answer :  10 days\n",
      "answer :  10 days\n",
      "------ 25 -------\n",
      "answer :  9 days\n",
      "answer :  26 days\n",
      "answer :  7 days\n",
      "answer :  10 days\n",
      "------ 26 -------\n",
      "answer :  2 days\n",
      "answer :  3 days\n",
      "answer :  6 days\n",
      "answer :  6 days\n",
      "------ 27 -------\n",
      "answer :  2 days\n",
      "answer :  7 days\n",
      "answer :  7 days\n",
      "answer :  7 days\n",
      "------ 28 -------\n",
      "answer :  2 days\n",
      "answer :  7 days\n",
      "answer :  7 days\n",
      "answer :  10 days\n",
      "------ 29 -------\n",
      "answer :  4 days\n",
      "answer :  6 days\n",
      "answer :  7 days\n",
      "answer :  8 days\n",
      "------ 30 -------\n",
      "answer :  1 day\n",
      "answer :  13 days\n",
      "answer :  4 days\n",
      "answer :  4 days\n",
      "------ 31 -------\n",
      "answer :  4 days\n",
      "answer :  25 days\n",
      "answer :  2 days\n",
      "answer :  29 days\n",
      "------ 32 -------\n",
      "answer :  4 days\n",
      "answer :  24 days\n",
      "answer :  3 days\n",
      "answer :  7 days\n",
      "------ 33 -------\n",
      "answer :  8 days\n",
      "answer :  4 days\n",
      "answer :  5 days\n",
      "answer :  3 days\n",
      "------ 34 -------\n",
      "answer :  1 day\n",
      "answer :  12 days\n",
      "answer :  20 days\n",
      "answer :  20 days\n",
      "------ 35 -------\n",
      "answer :  8 days\n",
      "answer :  4 days\n",
      "answer :  14 days\n",
      "answer :  14 days\n",
      "------ 36 -------\n",
      "answer :  7 days\n",
      "answer :  13 days\n",
      "answer :  7 days\n",
      "answer :  7 days\n",
      "------ 37 -------\n",
      "answer :  9 days\n",
      "answer :  12 days\n",
      "answer :  9 days\n",
      "answer :  9 days\n",
      "------ 38 -------\n",
      "answer :  3 days\n",
      "answer :  8 days\n",
      "answer :  12 days\n",
      "answer :  23 days\n",
      "------ 39 -------\n",
      "answer :  3 days\n",
      "answer :  1 day\n",
      "answer :  3 days\n",
      "answer :  3 days\n",
      "------ 40 -------\n",
      "answer :  1 day\n",
      "answer :  2 days\n",
      "answer :  1 day\n",
      "answer :  2 days\n",
      "------ 41 -------\n",
      "answer :  13 days\n",
      "answer :  4 days\n",
      "answer :  6 days\n",
      "answer :  20 days\n",
      "------ 42 -------\n",
      "answer :  2 days\n",
      "answer :  9 days\n",
      "answer :  1 day\n",
      "answer :  7 days\n",
      "------ 43 -------\n",
      "answer :  6 days\n",
      "answer :  4 days\n",
      "answer :  11 days\n",
      "answer :  10 days\n",
      "------ 44 -------\n",
      "answer :  3 days\n",
      "answer :  11 days\n",
      "answer :  10 days\n",
      "answer :  10 days\n",
      "------ 45 -------\n",
      "answer :  2 days\n",
      "answer :  9 days\n",
      "answer :  9 days\n",
      "answer :  7 days\n",
      "------ 46 -------\n",
      "answer :  2 days\n",
      "answer :  9 days\n",
      "answer :  10 days\n",
      "answer :  10 days\n",
      "------ 47 -------\n",
      "answer :  4 days\n",
      "answer :  5 days\n",
      "answer :  1 day\n",
      "answer :  8 days\n",
      "------ 48 -------\n",
      "answer :  4 days\n",
      "answer :  16 days\n",
      "answer :  16 days\n",
      "answer :  15 days\n",
      "------ 49 -------\n",
      "answer :  2 days\n",
      "answer :  7 days\n",
      "answer :  7 days\n",
      "answer :  7 days\n"
     ]
    }
   ],
   "source": [
    "longest_time_diff_pred_df = []\n",
    "longest_time_diff_pred_df_shuf = []\n",
    "longest_time_diff_pred_text1 = []\n",
    "longest_time_diff_pred_text2 = []\n",
    "\n",
    "system_prompt = 'You only answer in the following format: number days. Dont answer with a sentence.'\n",
    "user_prompt = 'What is the longest time interval between consecutive log in the visit log?'\n",
    "\n",
    "for i in range(50):\n",
    "    print('------', i, '-------')\n",
    "    # print('original df')\n",
    "    user_log = pd.read_csv(f'./data/subtasks_temporal/user_log/user_log_{i}.csv')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, user_log.to_string())\n",
    "    longest_time_diff_pred_df.append(output)\n",
    "    \n",
    "    # print('shuffle df')\n",
    "    user_log_shuffle = pd.read_csv(f'./data/subtasks_temporal/user_log_shuffle/user_log_shuffle_{i}.csv')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, user_log_shuffle.to_string())\n",
    "    longest_time_diff_pred_df_shuf.append(output)\n",
    "    \n",
    "    # print('text ver1')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, totext(user_log))\n",
    "    longest_time_diff_pred_text1.append(output)\n",
    "    \n",
    "    # print('text ver2')\n",
    "    output, tok = gpt_api(system_prompt, user_prompt, totextvisited(user_log))\n",
    "    longest_time_diff_pred_text2.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "ae805328-d7f6-4771-94b6-19b944d7b90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "df shuffle\n",
      "text1\n",
      "text2\n"
     ]
    }
   ],
   "source": [
    "longest_time_diff_pred_df_score = []\n",
    "\n",
    "print('df')\n",
    "for ai, bi in zip(longest_time_diff_ans, longest_time_diff_pred_df):\n",
    "    longest_time_diff_pred_df_score.append(int(str(ai).split(\", \")[0]) == int(str(bi).split(\" \")[0])) \n",
    "                                           # & (int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1])))\n",
    "    # if longest_time_diff_pred_df_cot_score[-1] == False :\n",
    "        # print(int(str(ai).split(\", \")[0]), int(str(bi).split(\", \")[0]), int(str(ai).split(\", \")[0]) == int(str(bi).split(\", \")[0]))\n",
    "        # print(int(str(ai).split(\", \")[1]), int(str(bi).split(\", \")[1]), int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1]))\n",
    "\n",
    "longest_time_diff_pred_df_shuf_score = []\n",
    "\n",
    "print('df shuffle')\n",
    "for ai, bi in zip(longest_time_diff_ans, longest_time_diff_pred_df_shuf):\n",
    "    longest_time_diff_pred_df_shuf_score.append(int(str(ai).split(\", \")[0]) == int(str(bi).split(\" \")[0])) \n",
    "                                                # & (int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1])))\n",
    "    # if longest_time_diff_pred_df_shuf_cot_score[-1] == False :\n",
    "        # print(int(str(ai).split(\", \")[0]), int(str(bi).split(\", \")[0]), int(str(ai).split(\", \")[0]) == int(str(bi).split(\", \")[0]))\n",
    "        # print(int(str(ai).split(\", \")[1]), int(str(bi).split(\", \")[1]), int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1]))\n",
    "    \n",
    "longest_time_diff_pred_text1_score = []\n",
    "\n",
    "print('text1')\n",
    "for ai, bi in zip(longest_time_diff_ans, longest_time_diff_pred_text1):\n",
    "    longest_time_diff_pred_text1_score.append(int(str(ai).split(\", \")[0]) == int(str(bi).split(\" \")[0])) \n",
    "                                              # & (int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1])))\n",
    "    # if str(bi).contains(\",\"):\n",
    "    #     longest_time_diff_pred_text1_score.append((int(str(ai).split(\", \")[0]) == int(str(bi).split(\", \")[0])) & \\\n",
    "    #                                            (int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1])))\n",
    "    # else:\n",
    "    #     longest_time_diff_pred_text1_score.append((int(str(ai).split(\", \")[0]) == int(str(bi).split(\" \")[0])) & \\\n",
    "    #                                            (int(str(ai).split(\", \")[1]) == int(str(bi).split(\" \")[1])))\n",
    "    # if longest_time_diff_pred_text1_cot_score[-1] == False :\n",
    "        # print(int(str(ai).split(\", \")[0]), int(str(bi).split(\", \")[0]), int(str(ai).split(\", \")[0]) == int(str(bi).split(\", \")[0]))\n",
    "        # print(int(str(ai).split(\", \")[1]), int(str(bi).split(\", \")[1]), int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1]))\n",
    "    \n",
    "longest_time_diff_pred_text2_score = []\n",
    "\n",
    "print('text2')\n",
    "for ai, bi in zip(longest_time_diff_ans, longest_time_diff_pred_text2):\n",
    "    longest_time_diff_pred_text2_score.append(int(str(ai).split(\", \")[0]) == int(str(bi).split(\" \")[0])) \n",
    "                                                  # & (int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1])))\n",
    "    # if longest_time_diff_pred_text2_cot_score[-1] == False :\n",
    "        # print(int(str(ai).split(\", \")[0]), int(str(bi).split(\", \")[0]), int(str(ai).split(\", \")[0]) == int(str(bi).split(\", \")[0]))\n",
    "        # print(int(str(ai).split(\", \")[1]), int(str(bi).split(\", \")[1]), int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "19dc5e85-60e1-47f6-abee-8940cbbaa93e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.08\n",
      "0.08\n"
     ]
    }
   ],
   "source": [
    "print(sum(longest_time_diff_pred_df_score)/50)\n",
    "print(sum(longest_time_diff_pred_df_shuf_score)/50)\n",
    "print(sum(longest_time_diff_pred_text1_score)/50)\n",
    "print(sum(longest_time_diff_pred_text2_score)/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe76494-407e-4d5c-a85e-6cb7713f435a",
   "metadata": {},
   "source": [
    "<font size=\"5\">prompt_maker - vary prompts and compare their functionalities</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f91f4fb0-0779-4930-9b9c-716736134196",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_maker(system_prompt, user_prompt, our_prompt_num, user_log, model=\"gpt-3.5-turbo\", verbose=False):\n",
    "\n",
    "    user_prompt += '\\n' + user_log\n",
    "    \n",
    "    if our_prompt_num == 1:\n",
    "        our_prompt = 'Analyze the given data appropriately for the question without missing any data.'\n",
    "    elif our_prompt_num == 2:\n",
    "        our_prompt = 'Reconstruct the given data to think and answer appropriately for the question without missing any data.'\n",
    "    elif our_prompt_num == 3:\n",
    "        our_prompt = 'Understand every visit log of the given data to answer the question.'\n",
    "    elif our_prompt_num == 4:\n",
    "        our_prompt = 'Use every visit log of the given data properly for the purpose of answering the question.'\n",
    "    elif our_prompt_num == 5:\n",
    "        our_prompt = 'Understand the question properly and think about the way to utilize every visit log of the given data.'\n",
    "    elif our_prompt_num == 6:\n",
    "        our_prompt = 'Reorganize the given N visit logs to match the questions and solve it step by step.'\n",
    "    \n",
    "    if verbose:\n",
    "      print(user_prompt)\n",
    "\n",
    "    messages = [\n",
    "                {\"role\": \"system\", \"content\": system_prompt}, \n",
    "                {\"role\": \"user\", \"content\": user_prompt + our_prompt}\n",
    "                ]\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "\n",
    "    output = response.choices[0].message.content\n",
    "    token = response.usage.total_tokens\n",
    "    \n",
    "    # print('answer : ', output)\n",
    "    # print('token : ', token)\n",
    "\n",
    "    return output, token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b9cc1c-88e5-4880-b2ea-8d6b46fffb8a",
   "metadata": {},
   "source": [
    "<font size=\"5\">Task 1 - prompt_maker w our_prompt 1 </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5cbcd260-5360-4c8b-a7d8-c15427b136b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "duration_pred_df1 = []\n",
    "duration_pred_df_shuf1 = []\n",
    "duration_pred_text11 = []\n",
    "duration_pred_text21 = []\n",
    "\n",
    "system_prompt = 'You only answer in the following format: year-month-day, year-month-day'\n",
    "user_prompt = 'What is the time range covered by this visit log?'\n",
    "our_prompt1 = 'Analyze the given data appropriately for the question without missing any data.'\n",
    "\n",
    "for i in range(50):\n",
    "    # print('original df')\n",
    "    user_log = pd.read_csv(f'./data/subtasks_temporal/user_log/user_log_{i}.csv')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, user_log.to_string())\n",
    "    duration_pred_df1.append(output)\n",
    "    \n",
    "    # print('shuffle df')\n",
    "    user_log_shuffle = pd.read_csv(f'./data/subtasks_temporal/user_log_shuffle/user_log_shuffle_{i}.csv')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, user_log_shuffle.to_string())\n",
    "    duration_pred_df_shuf1.append(output)\n",
    "    \n",
    "    # print('text ver1')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, totext(user_log))\n",
    "    duration_pred_text11.append(output)\n",
    "    \n",
    "    # print('text ver2')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, totextvisited(user_log))\n",
    "    duration_pred_text21.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "52d769e5-2c17-4c19-96dc-d960263c0424",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "df shuffle\n",
      "text1\n",
      "text2\n"
     ]
    }
   ],
   "source": [
    "duration_pred_df_score1 = []\n",
    "\n",
    "print('df')\n",
    "for ai, bi in zip(duration_ans, duration_pred_df1):\n",
    "    duration_pred_df_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "duration_pred_df_shuf_score1 = []\n",
    "\n",
    "print('df shuffle')\n",
    "for ai, bi in zip(duration_ans, duration_pred_df_shuf1):\n",
    "    duration_pred_df_shuf_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "duration_pred_df_text1_score1 = []\n",
    "\n",
    "print('text1')\n",
    "for ai, bi in zip(duration_ans, duration_pred_text11):\n",
    "    duration_pred_df_text1_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "duration_pred_df_text2_score1 = []\n",
    "\n",
    "print('text2')\n",
    "for ai, bi in zip(duration_ans, duration_pred_text21):\n",
    "    duration_pred_df_text2_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "adaf3402-7b60-42fc-9ce0-a0d8014f098d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "print(sum(duration_pred_df_score1)/50)\n",
    "print(sum(duration_pred_df_shuf_score1)/50)\n",
    "print(sum(duration_pred_df_text1_score1)/50)\n",
    "print(sum(duration_pred_df_text2_score1)/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9632e74-460d-4ec7-9464-8f8b45bb49a7",
   "metadata": {},
   "source": [
    "<font size=\"5\">Task 2 - prompt_maker w our_prompt 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8122d31e-6039-484d-bb1d-eb767079eb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_visited_date_pred_df1 = []\n",
    "most_visited_date_pred_df_shuf1 = []\n",
    "most_visited_date_pred_text11 = []\n",
    "most_visited_date_pred_text21 = []\n",
    "\n",
    "system_prompt = 'You only answer in the following format: year-month-day'\n",
    "user_prompt = 'When is the date with the most visits? However, if you have several days with the most visits, answer the earliest date.'\n",
    "# our_prompt1 = 'Analyze the given data appropriately for the question without missing any data.'\n",
    "\n",
    "for i in range(50):\n",
    "    # print('------', i, '-------')\n",
    "    # print('original df')\n",
    "    user_log = pd.read_csv(f'./data/subtasks_temporal/user_log/user_log_{i}.csv')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, user_log.to_string())\n",
    "    most_visited_date_pred_df1.append(output)\n",
    "    \n",
    "    # print('shuffle df')\n",
    "    user_log_shuffle = pd.read_csv(f'./data/subtasks_temporal/user_log_shuffle/user_log_shuffle_{i}.csv')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, user_log_shuffle.to_string())\n",
    "    most_visited_date_pred_df_shuf1.append(output)\n",
    "    \n",
    "    # print('text ver1')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, totext(user_log))\n",
    "    most_visited_date_pred_text11.append(output)\n",
    "    \n",
    "    # print('text ver2')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, totextvisited(user_log))\n",
    "    most_visited_date_pred_text21.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c9f85c59-4168-488b-9dfc-27ef0fbc8e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "2017-12-09 2017-12-08 False\n",
      "2017-12-20 2017-12-12 False\n",
      "2017-12-11 2017-12-10 False\n",
      "2017-12-07 2017-12-05 False\n",
      "2017-12-22 2017-12-13 False\n",
      "2017-12-16 2017-12-05 False\n",
      "2017-12-29 2017-12-17 False\n",
      "2018-01-04 2018-01-03 False\n",
      "2017-12-18 2017-12-16 False\n",
      "2018-01-03 2017-12-29 False\n",
      "2017-12-26 2017-12-04 False\n",
      "2017-12-23 2017-12-16 False\n",
      "2017-12-27 2017-12-26 False\n",
      "2017-12-20 2017-12-08 False\n",
      "2017-12-28 2017-12-22 False\n",
      "2017-12-31 2017-12-17 False\n",
      "2017-12-14 2017-12-05 False\n",
      "2017-12-09 2017-12-03 False\n",
      "2017-12-25 2017-12-16 False\n",
      "2017-12-19 2017-12-06 False\n",
      "2017-12-30 2017-12-21 False\n",
      "2017-12-28 2017-12-26 False\n",
      "2018-01-02 2017-12-18 False\n",
      "2018-01-05 2017-12-11 False\n",
      "2018-01-01 2017-12-13 False\n",
      "2018-01-01 2017-12-17 False\n",
      "2017-12-23 2017-12-05 False\n",
      "2017-12-15 2017-12-04 False\n",
      "2017-12-29 2017-12-25 False\n",
      "2018-01-05 2017-12-26 False\n",
      "2017-12-21 2017-12-15 False\n",
      "2017-12-27 2017-12-24 False\n",
      "2018-01-04 2017-12-21 False\n",
      "2017-12-28 2017-12-25 False\n",
      "2017-12-27 2017-12-19 False\n",
      "df\n",
      "2017-12-20 2017-12-29 False\n",
      "2017-12-11 2017-12-10 False\n",
      "2017-12-22 2017-12-15 False\n",
      "2017-12-18 2017-12-22 False\n",
      "2018-01-04 2018-01-05 False\n",
      "2017-12-26 2017-12-06 False\n",
      "2017-12-27 2017-12-26 False\n",
      "2017-12-28 2017-12-27 False\n",
      "2017-12-09 2017-12-03 False\n",
      "2017-12-25 2017-12-18 False\n",
      "2018-01-02 2017-12-31 False\n",
      "2017-12-05 2017-12-31 False\n",
      "2018-01-05 2017-12-11 False\n",
      "2018-01-01 2017-12-13 False\n",
      "2018-01-01 2017-12-17 False\n",
      "2017-12-21 2017-12-17 False\n",
      "2017-12-27 2017-12-26 False\n",
      "2018-01-04 2017-12-27 False\n",
      "2017-12-27 2017-12-20 False\n",
      "df\n",
      "2017-12-09 2017-12-08 False\n",
      "2017-12-20 2017-12-29 False\n",
      "2017-12-22 2017-12-15 False\n",
      "2017-12-23 The date with the most visits is 12-23-2017. False\n",
      "2017-12-26 2017-12-06 False\n",
      "2017-12-27 2017-12-26 False\n",
      "2017-12-20 2017-12-26 False\n",
      "2017-12-28 2017-12-23 False\n",
      "2017-12-31 2017-12-21 False\n",
      "2017-12-30 2017-12-28 False\n",
      "2017-12-28 2017-12-26 False\n",
      "2018-01-02 2017-12-31 False\n",
      "2017-12-05 2017-12-07 False\n",
      "2018-01-05 2017-12-11 False\n",
      "2018-01-01 2017-12-13 False\n",
      "2018-01-01 2017-12-31 False\n",
      "2017-12-29 2017-12-28 False\n",
      "2018-01-05 2017-12-26 False\n",
      "2017-12-21 2017-12-19 False\n",
      "2017-12-27 2017-12-24 False\n",
      "2018-01-04 2017-12-27 False\n",
      "df\n",
      "2017-12-09 2017-12-08 False\n",
      "2017-12-20 2017-12-29 False\n",
      "2018-01-04 The date with the most visits is 01-04-2018. False\n",
      "2018-01-03 2017-12-29 False\n",
      "2017-12-26 2017-12-04 False\n",
      "2017-12-27 2017-12-26 False\n",
      "2017-12-20 2017-12-26 False\n",
      "2017-12-28 2017-12-23 False\n",
      "2017-12-31 2017-12-21 False\n",
      "2017-12-09 2017-12-03 False\n",
      "2017-12-19 2017-12-06 False\n",
      "2017-12-28 2017-12-26 False\n",
      "2018-01-02 2017-12-18 False\n",
      "2018-01-05 The date with the most visits is 12-11-2017. False\n",
      "2018-01-01 2017-12-13 False\n",
      "2018-01-01 2017-12-31 False\n",
      "2017-12-15 The date with the most visits is 12-27-2017. False\n",
      "2018-01-05 2017-12-26 False\n",
      "2017-12-21 2017-12-19 False\n",
      "2017-12-27 2017-12-24 False\n",
      "2018-01-04 2017-12-24 False\n",
      "2017-12-28 2017-12-25 False\n"
     ]
    }
   ],
   "source": [
    "most_visited_date_pred_df_score1 = []\n",
    "\n",
    "print('df')\n",
    "for ai, bi in zip(most_visited_date_ans, most_visited_date_pred_df1):\n",
    "    most_visited_date_pred_df_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_date_pred_df_shuf_score1 = []\n",
    "\n",
    "print('df')\n",
    "for ai, bi in zip(most_visited_date_ans, most_visited_date_pred_df_shuf1):\n",
    "    most_visited_date_pred_df_shuf_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_date_pred_text1_score1 = []\n",
    "\n",
    "print('df')\n",
    "for ai, bi in zip(most_visited_date_ans, most_visited_date_pred_text11):\n",
    "    most_visited_date_pred_text1_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_date_pred_text2_score1 = []\n",
    "\n",
    "print('df')\n",
    "for ai, bi in zip(most_visited_date_ans, most_visited_date_pred_text21):\n",
    "    most_visited_date_pred_text2_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f84183a2-2325-41ea-ad9f-92c72d7c3df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3\n",
      "0.62\n",
      "0.58\n",
      "0.56\n"
     ]
    }
   ],
   "source": [
    "print(sum(most_visited_date_pred_df_score1)/50)\n",
    "print(sum(most_visited_date_pred_df_shuf_score1)/50)\n",
    "print(sum(most_visited_date_pred_text1_score1)/50)\n",
    "print(sum(most_visited_date_pred_text2_score1)/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682d8067-adb1-49c5-b942-607208e8d448",
   "metadata": {},
   "source": [
    "<font size=\"5\">Task 3 - prompt_maker w our_prompt 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "311e2a30-51a2-4b68-818e-c74b08bb2d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_visited_hour_pred_df1 = []\n",
    "most_visited_hour_pred_df_shuf1 = []\n",
    "most_visited_hour_pred_text11 = []\n",
    "most_visited_hour_pred_text21 = []\n",
    "\n",
    "system_prompt = 'You only answer in the one number: hour. Dont answer with a sentence.'\n",
    "user_prompt = 'When is the hour with the most visits? However, if you have several hour with the most visits, answer the earliest hour.'\n",
    "our_prompt1 = 'Analyze the given data appropriately for the question without missing any data.'\n",
    "\n",
    "for i in range(50):\n",
    "    # print('------', i, '-------')\n",
    "    # print('original df')\n",
    "    user_log = pd.read_csv(f'./data/subtasks_temporal/user_log/user_log_{i}.csv')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, user_log.to_string())\n",
    "    most_visited_hour_pred_df.append(output)\n",
    "    \n",
    "    # print('shuffle df')\n",
    "    user_log_shuffle = pd.read_csv(f'./data/subtasks_temporal/user_log_shuffle/user_log_shuffle_{i}.csv')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, user_log_shuffle.to_string())\n",
    "    most_visited_hour_pred_df_shuf.append(output)\n",
    "    \n",
    "    # print('text ver1')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, totext(user_log))\n",
    "    most_visited_hour_pred_text1.append(output)\n",
    "    \n",
    "    # print('text ver2')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, totextvisited(user_log))\n",
    "    most_visited_hour_pred_text2.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "19b6d1ba-2494-41e2-b5dc-821881234a52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "df shuffle\n",
      "text1\n",
      "text2\n"
     ]
    }
   ],
   "source": [
    "most_visited_hour_pred_df_score1 = []\n",
    "\n",
    "print('df')\n",
    "for ai, bi in zip(list(map(int, most_visited_hour_ans)), list(map(int, most_visited_hour_pred_df1))):\n",
    "    most_visited_hour_pred_df_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_hour_pred_df_shuf_score1 = []\n",
    "\n",
    "print('df shuffle')\n",
    "for ai, bi in zip(list(map(int, most_visited_hour_ans)), list(map(int, most_visited_hour_pred_df_shuf1))):\n",
    "    most_visited_hour_pred_df_shuf_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_hour_pred_text1_score1 = []\n",
    "\n",
    "print('text1')\n",
    "for ai, bi in zip(list(map(int, most_visited_hour_ans)), list(map(int, most_visited_hour_pred_text11))):\n",
    "    most_visited_hour_pred_text1_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_hour_pred_text2_score1 = []\n",
    "\n",
    "print('text2')\n",
    "for ai, bi in zip(list(map(int, most_visited_hour_ans)), list(map(int, most_visited_hour_pred_text21))):\n",
    "    most_visited_hour_pred_text2_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "fe807d88-1190-48a0-9bf0-9300e36b6d26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "print(sum(most_visited_hour_pred_df_score1)/50)\n",
    "print(sum(most_visited_hour_pred_df_shuf_score1)/50)\n",
    "print(sum(most_visited_hour_pred_text1_score1)/50)\n",
    "print(sum(most_visited_hour_pred_text2_score1)/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2275c8b0-1e31-4516-bcd1-99f257e2abfb",
   "metadata": {},
   "source": [
    "<font size=\"5\">Task 4 - prompt_maker w our_prompt 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "c90cbb81-e236-4fe3-9458-33332b4b8007",
   "metadata": {},
   "outputs": [],
   "source": [
    "most_visited_timestamp_pred_df1 = []\n",
    "most_visited_timestamp_pred_df_shuf1 = []\n",
    "most_visited_timestamp_pred_text11 = []\n",
    "most_visited_timestamp_pred_text21 = []\n",
    "\n",
    "system_prompt = 'You only answer in the following format: year-mont-day hour. Dont answer with a sentence.'\n",
    "user_prompt = 'When is the timestamp with the most visits? However, if you have several timstamp with the most visits, answer the earliest one.'\n",
    "our_prompt1 = 'Analyze the given data appropriately for the question without missing any data.'\n",
    "\n",
    "for i in range(50):\n",
    "    # print('------', i, '-------')\n",
    "    # print('original df')\n",
    "    user_log = pd.read_csv(f'./data/subtasks_temporal/user_log/user_log_{i}.csv')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, user_log.to_string())\n",
    "    most_visited_timestamp_pred_df1.append(output)\n",
    "    \n",
    "    # print('shuffle df')\n",
    "    user_log_shuffle = pd.read_csv(f'./data/subtasks_temporal/user_log_shuffle/user_log_shuffle_{i}.csv')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, user_log_shuffle.to_string())\n",
    "    most_visited_timestamp_pred_df_shuf1.append(output)\n",
    "    \n",
    "    # print('text ver1')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, totext(user_log))\n",
    "    most_visited_timestamp_pred_text11.append(output)\n",
    "    \n",
    "    # print('text ver2')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, totextvisited(user_log))\n",
    "    most_visited_timestamp_pred_text21.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "4775711d-f44b-4f42-9225-6601de4fd59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "2017-12-09 14:00:00 2017-12-08 18:00:00 False\n",
      "2017-12-20 23:00:00 2017-12-11 11:00:00 False\n",
      "2017-12-11 16:00:00 2017-12-10 17:00:00 False\n",
      "2017-12-23 17:00:00 2017-12-16 14:00:00 False\n",
      "2017-12-13 23:00:00 2017-12-05 12:00:00 False\n",
      "2017-12-14 18:00:00 2017-12-13 13:00:00 False\n",
      "2017-12-16 17:00:00 2017-12-05 18:00:00 False\n",
      "2017-12-29 12:00:00 2017-12-17 14:00:00 False\n",
      "2017-12-18 12:00:00 2017-12-07 16:00:00 False\n",
      "2018-01-05 22:00:00 2018-01-03 19:00:00 False\n",
      "2017-12-23 18:00:00 2017-12-23 14:00:00 False\n",
      "2017-12-19 15:00:00 2017-12-16 21:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-29 17:00:00 False\n",
      "2017-12-06 21:00:00 2017-12-04 17:00:00 False\n",
      "2017-12-23 14:00:00 2017-12-16 17:00:00 False\n",
      "2017-12-20 07:00:00 2017-12-08 20:00:00 False\n",
      "2017-12-20 19:00:00 2017-12-20 15:00:00 False\n",
      "2017-12-23 22:00:00 2017-12-21 08:00:00 False\n",
      "2017-12-14 15:00:00 2017-12-05 15:00:00 False\n",
      "2017-12-25 13:00:00 2017-12-16 22:00:00 False\n",
      "2017-12-19 22:00:00 2017-12-06 12:00:00 False\n",
      "2017-12-30 15:00:00 2017-12-20 11:00:00 False\n",
      "2017-12-13 06:00:00 2017-12-09 09:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-11 19:00:00 False\n",
      "2017-12-30 17:00:00 2017-12-22 18:00:00 False\n",
      "2018-01-01 16:00:00 2017-12-13 16:00:00 False\n",
      "2018-01-01 19:00:00 2017-12-17 17:00:00 False\n",
      "2017-12-05 12:00:00 2017-12-04 15:00:00 False\n",
      "2017-12-29 21:00:00 2017-12-25 17:00:00 False\n",
      "2017-12-22 19:00:00 2017-12-22 16:00:00 False\n",
      "2017-12-29 19:00:00 2017-12-09 16:00:00 False\n",
      "2018-01-05 17:00:00 2017-12-26 08:00:00 False\n",
      "2017-12-25 14:00:00 2017-12-25 13:00:00 False\n",
      "2017-12-29 15:00:00 2017-12-24 14:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-21 19:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-25 09:00:00 False\n",
      "2017-12-22 18:00:00 2017-12-19 19:00:00 False\n",
      "df shuffle\n",
      "2017-12-20 23:00:00 2017-12-19 18:00:00 False\n",
      "2017-12-11 16:00:00 2017-12-11 15:00:00 False\n",
      "2017-12-23 17:00:00 2017-12-23 21:00:00 False\n",
      "2017-12-13 23:00:00 2017-12-07 18:00:00 False\n",
      "2017-12-14 18:00:00 2017-12-15 14:00:00 False\n",
      "2017-12-16 17:00:00 2017-12-16 20:00:00 False\n",
      "2017-12-18 12:00:00 2017-12-18 13:00:00 False\n",
      "2017-12-23 18:00:00 2017-12-23 20:00:00 False\n",
      "2017-12-19 15:00:00 2017-12-18 12:00:00 False\n",
      "2017-12-06 21:00:00 2017-12-16 18:00:00 False\n",
      "2017-12-20 07:00:00 2017-12-20 17:00:00 False\n",
      "2017-12-23 16:00:00 2017-12-27 21:00:00 False\n",
      "2017-12-23 22:00:00 2017-12-21 08:00:00 False\n",
      "2017-12-03 18:00:00 2017-12-12 12:00:00 False\n",
      "2017-12-25 13:00:00 2017-12-18 17:00:00 False\n",
      "2017-12-18 16:00:00 2017-12-31 19:00:00 False\n",
      "2017-12-05 18:00:00 2017-12-06 17:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-11 19:00:00 False\n",
      "2017-12-30 17:00:00 2017-12-30 19:00:00 False\n",
      "2018-01-01 16:00:00 2017-12-13 16:00:00 False\n",
      "2018-01-01 19:00:00 2017-12-17 17:00:00 False\n",
      "2017-12-05 09:00:00 2017-12-23 11:00:00 False\n",
      "2017-12-05 12:00:00 2017-12-15 11:00:00 False\n",
      "2017-12-29 19:00:00 2017-12-09 17:00:00 False\n",
      "2018-01-05 17:00:00 2017-12-27 09:00:00 False\n",
      "2017-12-15 23:00:00 2017-12-21 23:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-27 17:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-31 16:00:00 False\n",
      "2017-12-02 18:00:00 2017-12-02 17:00:00 False\n",
      "2017-12-22 18:00:00 2017-12-20 21:00:00 False\n",
      "text1\n",
      "2017-12-09 14:00:00 2017-12-08 20:00:00 False\n",
      "2017-12-20 23:00:00 2017-12-12 21:00:00 False\n",
      "2017-12-11 16:00:00 2017-12-10 17:00:00 False\n",
      "2017-12-13 23:00:00 2017-12-07 14:00:00 False\n",
      "2017-12-14 18:00:00 2017-12-15 11:00:00 False\n",
      "2017-12-16 17:00:00 2017-12-05 16:00:00 False\n",
      "2017-12-29 12:00:00 2017-12-17 14:00:00 False\n",
      "2018-01-05 22:00:00 2018-01-03 19:00:00 False\n",
      "2017-12-19 15:00:00 2017-12-16 21:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-29 17:00:00 False\n",
      "2017-12-23 14:00:00 2017-12-16 17:00:00 False\n",
      "2017-12-20 07:00:00 2017-12-08 21:00:00 False\n",
      "2017-12-20 19:00:00 2017-12-20 15:00:00 False\n",
      "2017-12-23 16:00:00 2017-12-22 21:00:00 False\n",
      "2017-12-23 22:00:00 2017-12-21 08:00:00 False\n",
      "2017-12-14 15:00:00 2017-12-05 17:00:00 False\n",
      "2017-12-19 22:00:00 2017-12-19 12:00:00 False\n",
      "2017-12-30 15:00:00 2017-12-20 18:00:00 False\n",
      "2017-12-13 06:00:00 2017-12-09 09:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-11 19:00:00 False\n",
      "2017-12-30 17:00:00 2017-12-30 22:00:00 False\n",
      "2018-01-01 16:00:00 2017-12-13 16:00:00 False\n",
      "2018-01-01 19:00:00 2017-12-17 17:00:00 False\n",
      "2017-12-05 09:00:00 2017-12-07 09:00:00 False\n",
      "2017-12-05 12:00:00 2017-12-04 12:00:00 False\n",
      "2017-12-29 21:00:00 2017-12-25 17:00:00 False\n",
      "2017-12-22 19:00:00 2017-12-22 16:00:00 False\n",
      "2017-12-29 19:00:00 2017-12-09 16:00:00 False\n",
      "2018-01-05 17:00:00 2017-12-26 18:00:00 False\n",
      "2017-12-15 23:00:00 2017-12-19 08:00:00 False\n",
      "2017-12-25 14:00:00 2017-12-25 13:00:00 False\n",
      "2017-12-29 15:00:00 2017-12-24 15:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-21 20:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-25 15:00:00 False\n",
      "2017-12-22 18:00:00 2017-12-19 19:00:00 False\n",
      "text2\n",
      "2017-12-09 14:00:00 2017-12-08 20:00:00 False\n",
      "2017-12-20 23:00:00 2017-12-12 19:00:00 False\n",
      "2017-12-13 23:00:00 2017-12-07 12:00:00 False\n",
      "2017-12-14 18:00:00 2017-12-13 13:00:00 False\n",
      "2017-12-16 17:00:00 2017-12-05 16:00:00 False\n",
      "2017-12-29 12:00:00 2017-12-18 13:00:00 False\n",
      "2018-01-05 22:00:00 2018-01-03 19:00:00 False\n",
      "2017-12-19 15:00:00 2017-12-16 21:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-29 17:00:00 False\n",
      "2017-12-06 21:00:00 2017-12-04 17:00:00 False\n",
      "2017-12-23 14:00:00 2017-12-16 17:00:00 False\n",
      "2017-12-20 07:00:00 2017-12-08 20:00:00 False\n",
      "2017-12-23 16:00:00 2017-12-22 20:00:00 False\n",
      "2017-12-23 22:00:00 2017-12-21 08:00:00 False\n",
      "2017-12-14 15:00:00 2017-12-14 17:00:00 False\n",
      "2017-12-25 13:00:00 2017-12-16 22:00:00 False\n",
      "2017-12-19 22:00:00 2017-12-06 18:00:00 False\n",
      "2017-12-30 15:00:00 2017-12-20 11:00:00 False\n",
      "2017-12-13 06:00:00 2017-12-09 09:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-11 07:00:00 False\n",
      "2017-12-30 17:00:00 2017-12-30 19:00:00 False\n",
      "2018-01-01 16:00:00 2017-12-13 16:00:00 False\n",
      "2018-01-01 19:00:00 2017-12-17 17:00:00 False\n",
      "2017-12-05 09:00:00 2017-12-07 09:00:00 False\n",
      "2017-12-05 12:00:00 2017-12-27 16:00:00 False\n",
      "2017-12-29 21:00:00 2017-12-25 17:00:00 False\n",
      "2017-12-29 19:00:00 2017-12-09 16:00:00 False\n",
      "2018-01-05 17:00:00 2017-12-26 08:00:00 False\n",
      "2017-12-29 15:00:00 2017-12-24 15:00:00 False\n",
      "2018-01-04 18:00:00 2017-12-21 20:00:00 False\n",
      "2018-01-03 07:00:00 2017-12-25 15:00:00 False\n",
      "2017-12-22 18:00:00 2017-12-19 07:00:00 False\n"
     ]
    }
   ],
   "source": [
    "most_visited_timestamp_pred_df_score1 = []\n",
    "\n",
    "print('df')\n",
    "for ai, bi in zip([dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_ans], [dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_pred_df1]):\n",
    "    most_visited_timestamp_pred_df_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_timestamp_pred_df_shuf_score1 = []\n",
    "\n",
    "print('df shuffle')\n",
    "for ai, bi in zip([dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_ans], [dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_pred_df_shuf1]):\n",
    "    most_visited_timestamp_pred_df_shuf_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_timestamp_pred_text1_score1 = []\n",
    "\n",
    "print('text1')\n",
    "for ai, bi in zip([dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_ans], [dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_pred_text11]):\n",
    "    most_visited_timestamp_pred_text1_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)\n",
    "    \n",
    "most_visited_timestamp_pred_text2_score1 = []\n",
    "\n",
    "print('text2')\n",
    "for ai, bi in zip([dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_ans], [dt.strptime(date_str, \"%Y-%m-%d %H\") for date_str in most_visited_timestamp_pred_text21]):\n",
    "    most_visited_timestamp_pred_text2_score1.append(ai == bi)\n",
    "    if ai != bi : print(ai, bi, ai==bi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "77df5692-da9b-4244-ac5d-f4b6e7ed019c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26\n",
      "0.4\n",
      "0.3\n",
      "0.36\n"
     ]
    }
   ],
   "source": [
    "print(sum(most_visited_timestamp_pred_df_score1)/50)\n",
    "print(sum(most_visited_timestamp_pred_df_shuf_score1)/50)\n",
    "print(sum(most_visited_timestamp_pred_text1_score1)/50)\n",
    "print(sum(most_visited_timestamp_pred_text2_score1)/50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac91679e-0579-47e8-943a-88a5c15f188e",
   "metadata": {},
   "source": [
    "<font size=\"5\">Task 5 - prompt_maker w our_prompt 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "6eecfc76-3034-407f-ade0-a13e1b382bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "longest_time_diff_pred_df1 = []\n",
    "longest_time_diff_pred_df_shuf1 = []\n",
    "longest_time_diff_pred_text11 = []\n",
    "longest_time_diff_pred_text21 = []\n",
    "\n",
    "system_prompt = 'You only answer in the following format: number days. Dont answer with a sentence.'\n",
    "user_prompt = 'What is the longest time interval between consecutive log in the visit log?'\n",
    "our_prompt1 = 'Analyze the given data appropriately for the question without missing any data.'\n",
    "\n",
    "for i in range(50):\n",
    "    # print('------', i, '-------')\n",
    "    # print('original df')\n",
    "    user_log = pd.read_csv(f'./data/subtasks_temporal/user_log/user_log_{i}.csv')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, user_log.to_string())\n",
    "    longest_time_diff_pred_df1.append(output)\n",
    "    \n",
    "    # print('shuffle df')\n",
    "    user_log_shuffle = pd.read_csv(f'./data/subtasks_temporal/user_log_shuffle/user_log_shuffle_{i}.csv')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, user_log_shuffle.to_string())\n",
    "    longest_time_diff_pred_df_shuf1.append(output)\n",
    "    \n",
    "    # print('text ver1')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, totext(user_log))\n",
    "    longest_time_diff_pred_text11.append(output)\n",
    "    \n",
    "    # print('text ver2')\n",
    "    output, tok = prompt_maker(system_prompt, user_prompt, 1, totextvisited(user_log))\n",
    "    longest_time_diff_pred_text21.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "53eeec0f-2af3-4ebc-b82a-17715057cd7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\n",
      "df shuffle\n",
      "text1\n",
      "text2\n"
     ]
    }
   ],
   "source": [
    "longest_time_diff_pred_df_score1 = []\n",
    "\n",
    "print('df')\n",
    "for ai, bi in zip(longest_time_diff_ans, longest_time_diff_pred_df1):\n",
    "    longest_time_diff_pred_df_score1.append(int(str(ai).split(\", \")[0]) == int(str(bi).split(\" \")[0])) \n",
    "                                            # & (int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1])))\n",
    "    # if longest_time_diff_pred_df_cot_score[-1] == False :\n",
    "        # print(int(str(ai).split(\", \")[0]), int(str(bi).split(\", \")[0]), int(str(ai).split(\", \")[0]) == int(str(bi).split(\", \")[0]))\n",
    "        # print(int(str(ai).split(\", \")[1]), int(str(bi).split(\", \")[1]), int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1]))\n",
    "\n",
    "longest_time_diff_pred_df_shuf_score1 = []\n",
    "\n",
    "print('df shuffle')\n",
    "for ai, bi in zip(longest_time_diff_ans, longest_time_diff_pred_df_shuf1):\n",
    "    longest_time_diff_pred_df_shuf_score1.append(int(str(ai).split(\", \")[0]) == int(str(bi).split(\" \")[0])) \n",
    "                                                 # & (int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1])))\n",
    "    # if longest_time_diff_pred_df_shuf_cot_score[-1] == False :\n",
    "        # print(int(str(ai).split(\", \")[0]), int(str(bi).split(\", \")[0]), int(str(ai).split(\", \")[0]) == int(str(bi).split(\", \")[0]))\n",
    "        # print(int(str(ai).split(\", \")[1]), int(str(bi).split(\", \")[1]), int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1]))\n",
    "    \n",
    "longest_time_diff_pred_text1_score1 = []\n",
    "\n",
    "print('text1')\n",
    "for ai, bi in zip(longest_time_diff_ans, longest_time_diff_pred_text11):\n",
    "    longest_time_diff_pred_text1_score1.append(int(str(ai).split(\", \")[0]) == int(str(bi).split(\" \")[0])) \n",
    "                                               # & \\(int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1])))\n",
    "    # if longest_time_diff_pred_text1_cot_score[-1] == False :\n",
    "        # print(int(str(ai).split(\", \")[0]), int(str(bi).split(\", \")[0]), int(str(ai).split(\", \")[0]) == int(str(bi).split(\", \")[0]))\n",
    "        # print(int(str(ai).split(\", \")[1]), int(str(bi).split(\", \")[1]), int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1]))\n",
    "    \n",
    "longest_time_diff_pred_text2_score1 = []\n",
    "\n",
    "print('text2')\n",
    "for ai, bi in zip(longest_time_diff_ans, longest_time_diff_pred_text21):\n",
    "    longest_time_diff_pred_text2_score1.append(int(str(ai).split(\", \")[0]) == int(str(bi).split(\" \")[0])) \n",
    "                                               # & (int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1])))\n",
    "    # if longest_time_diff_pred_text2_cot_score[-1] == False :\n",
    "        # print(int(str(ai).split(\", \")[0]), int(str(bi).split(\", \")[0]), int(str(ai).split(\", \")[0]) == int(str(bi).split(\", \")[0]))\n",
    "        # print(int(str(ai).split(\", \")[1]), int(str(bi).split(\", \")[1]), int(str(ai).split(\", \")[1]) == int(str(bi).split(\", \")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d8a80cc9-c17d-4274-bfdc-f917c662d83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.26\n",
      "0.04\n",
      "0.2\n",
      "0.12\n"
     ]
    }
   ],
   "source": [
    "print(sum(longest_time_diff_pred_df_score1)/50)\n",
    "print(sum(longest_time_diff_pred_df_shuf_score1)/50)\n",
    "print(sum(longest_time_diff_pred_text1_score1)/50)\n",
    "print(sum(longest_time_diff_pred_text2_score1)/50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af03b168-a236-428f-a8b1-f73d5ce270db",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
